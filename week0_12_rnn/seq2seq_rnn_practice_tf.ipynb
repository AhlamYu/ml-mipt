{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzHVHe5-FOIK"
   },
   "source": [
    "## seq2seq practice\n",
    "### Generating names with recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-ZglR7cFOIT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtCPQo7mFOIV"
   },
   "source": [
    "# Our data\n",
    "The dataset contains ~8k earthling names from different cultures, all in latin transcript.\n",
    "\n",
    "This notebook has been designed so as to allow you to quickly swap names for something similar: deep learning article titles, IKEA furniture, pokemon names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6mKTbBuFOIV"
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell in Colab\n",
    "\n",
    "# ! wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/master/week0_10_embeddings_and_seq2seq/names -O names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbdiLylNFOIW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "start_token = \" \"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token + line for line in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYc-mj9nFOIW",
    "outputId": "4a31fb85-ba44-4495-bfe5-83cb5ca09be8"
   },
   "outputs": [],
   "source": [
    "print ('n samples = ', len(names))\n",
    "for x in names[::1000]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "3my7dVoQFOIX",
    "outputId": "7d46fba0-e5cf-4469-d4a1-2f8ff0031f3a"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(map(len, names))\n",
    "print(\"max length =\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.hist(list(map(len, names)),bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnjJOFE6FOIY"
   },
   "source": [
    "# Text processing\n",
    "\n",
    "First we need next to collect a \"vocabulary\" of all unique tokens i.e. unique characters. We can then encode inputs as a sequence of character ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxFWuUabGbS8",
    "outputId": "bcf32280-a610-44b8-ea52-cf9bd2f3449a"
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    if \" \" in name[1:]:\n",
    "        print(name[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVWSa6PYFOIZ",
    "outputId": "ab71dd85-b83f-42f6-ddea-be7bb2de031c"
   },
   "outputs": [],
   "source": [
    "#all unique characters go here\n",
    "tokens = set(\"\".join(names))\n",
    "\n",
    "tokens = list(tokens)\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print ('num_tokens = ', num_tokens)\n",
    "\n",
    "assert 25 < num_tokens < 30, \"Names should contain within 25 and 30 unique tokens depending on encoding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QMkeiK-FOIZ"
   },
   "source": [
    "### Convert characters to integers\n",
    "\n",
    "To train our neural network, we'll need to replace characters with their indices in tokens list.\n",
    "\n",
    "Let's compose a dictionary that does this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe6NXKrrFOIa"
   },
   "outputs": [],
   "source": [
    "token_to_id = <dictionary of symbol -> its identifier (index in tokens list)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ggsnp6gFOIa"
   },
   "outputs": [],
   "source": [
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "\n",
    "for i in range(num_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGGul8xsFOIb"
   },
   "outputs": [],
   "source": [
    "def to_matrix(names, max_len=None, pad=token_to_id[' '], dtype='int32', batch_first = True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, names))\n",
    "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        line_ix = [token_to_id[c] for c in names[i]]\n",
    "        names_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        names_ix = np.transpose(names_ix)\n",
    "\n",
    "    return names_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMzGW--eFOIc"
   },
   "outputs": [],
   "source": [
    "#Example: cast 4 random names to matrices, pad with zeros\n",
    "print('\\n'.join(names[::2000]))\n",
    "print(to_matrix(names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPyI90icj3um"
   },
   "source": [
    "### Create Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIlHWzKnoC7h"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IR4IHz8Sj25e"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 16\n",
    "\n",
    "vectorized_names = #YOUR CODE HERE\n",
    "dataset = tf.data.Dataset.from_tensor_slices(vectorized_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHtMY6OPkAAB"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    #YOUR CODE HERE\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = dataset.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhTzbSBWoJml"
   },
   "outputs": [],
   "source": [
    "sample_x, sample_y = next(iter(dataset))\n",
    "\n",
    "sample_x, sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0ORO6v9kC4q"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(1024)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWpB61cyFOIc"
   },
   "source": [
    "# Recurrent neural network\n",
    "\n",
    "We can rewrite recurrent neural network as a consecutive application of dense layer to input $x_t$ and previous rnn state $h_t$. This is exactly what we're gonna do now.\n",
    "\n",
    "Since we're training a language model, there should also be:\n",
    "* An embedding layer that converts character id x_t to a vector.\n",
    "* An output layer that predicts probabilities of next phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jt_AHaIRFOId"
   },
   "outputs": [],
   "source": [
    "class RNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=128):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(num_tokens, embedding_size)\n",
    "        self.rnn_update = tf.keras.layers.Dense(rnn_num_units)\n",
    "        self.rnn_to_logits = tf.keras.layers.Dense(num_tokens)\n",
    "    \n",
    "    def rnn_cell(self, x_current_emb, h_prev, training=False):\n",
    "        # compute next hidden state using self.rnn_update\n",
    "        # hint: use tf.concat(..., axis=...) for concatenation\n",
    "        x_and_h = #YOUR CODE HERE\n",
    "        h_next = #YOUR CODE HERE\n",
    "        \n",
    "        h_next = #YOUR CODE HERE\n",
    "        \n",
    "        assert h_next.shape == h_prev.shape\n",
    "\n",
    "        #compute logits for next character probs\n",
    "        logits =  #YOUR CODE\n",
    "\n",
    "        return h_next, tf.nn.log_softmax(logits, axis=-1)\n",
    "        \n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        \"\"\"\n",
    "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
    "        We'll call it repeatedly to produce the whole sequence.\n",
    "        \n",
    "        :param x: batch of character ids, containing vector of int64\n",
    "        :param h_prev: previous rnn hidden states, containing matrix [batch, rnn_num_units] of float32\n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "        # get vector embedding of x\n",
    "        x_emb = #YOUR CODE\n",
    "        \n",
    "        if states is None:\n",
    "            h_prev = self.initial_state(x.shape[0])\n",
    "        else:\n",
    "            h_prev = states\n",
    "        \n",
    "        logprobs = tf.TensorArray(tf.float32, size=x.shape[1])\n",
    "        states = tf.TensorArray(tf.float32, size=x.shape[1])\n",
    "\n",
    "        x_emb_transposed = tf.transpose(x_emb, perm=[1, 0, 2])\n",
    "\n",
    "        for idx in tf.range(x.shape[1]):\n",
    "            # use here your one-step code\n",
    "            h_prev, logp_next = self.rnn_cell(\n",
    "                x_emb_transposed[idx], h_prev, training=training)  \n",
    "            logprobs = logprobs.write(idx, logp_next)\n",
    "            states = states.write(idx, h_prev)\n",
    "\n",
    "        if return_state:\n",
    "            return tf.transpose(states.stack(), [1, 0, 2]), tf.transpose(logprobs.stack(), [1, 0, 2])\n",
    "        else:\n",
    "            return tf.transpose(logprobs.stack(), [1, 0, 2])\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
    "        return tf.zeros((batch_size, self.num_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLt3zb4nFOIe"
   },
   "outputs": [],
   "source": [
    "model = RNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdVWXSYsnqOs"
   },
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7YngXQ7FOIg"
   },
   "source": [
    "### The training loop\n",
    "\n",
    "We train our char-rnn exactly the same way we train any deep learning model: by minibatch sgd.\n",
    "\n",
    "The only difference is that this time we sample strings, not images or sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sgzwr-Pre2JE"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65XUMLE3fJaD"
   },
   "outputs": [],
   "source": [
    "# EPOCHS = 500\n",
    "EPOCHS = 100\n",
    "\n",
    "history = model.fit(\n",
    "    dataset, epochs=EPOCHS, \n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=\"best_model.h5\", save_best_only=True), \n",
    "        tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-4, patience=5)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENyNS_L2rJmw"
   },
   "outputs": [],
   "source": [
    "loss_history = history.history['loss']\n",
    "epochs_range = range(len(loss_history))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs_range, loss_history)\n",
    "plt.title('Training loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGCckm3fFOIh"
   },
   "source": [
    "### RNN: sampling\n",
    "Once we've trained our network a bit, let's get to actually generating stuff. \n",
    "All we need is the single rnn step function you have defined in `char_rnn.forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETQKEPlRtVwM"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):  \n",
    "        # Run the model, get states and predictions\n",
    "        states, predicted_logits = #YOUR CODE\n",
    "\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits_last_step = #YOUR CODE\n",
    "        predicted_logits_last_step = predicted_logits_last_step / self.temperature\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "\n",
    "        # get the last model state\n",
    "        state = #YOUR CODE\n",
    "\n",
    "        return state, predicted_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmvRV2y0xbE8"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9G8f8pXCy7pA"
   },
   "outputs": [],
   "source": [
    "seed_phrase = ' Sherl'.lower()\n",
    "seed_phrase_tensor = [token_to_id[token] for token in seed_phrase]\n",
    "\n",
    "seed_phrase_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJFvIxlLxh8B"
   },
   "outputs": [],
   "source": [
    "next_char = tf.convert_to_tensor([seed_phrase_tensor])\n",
    "\n",
    "result = [next_char]\n",
    "last_state = None\n",
    "\n",
    "for n in range(MAX_LENGTH - len(seed_phrase)):\n",
    "  last_state, next_char = one_step_model.generate_one_step(next_char, states=last_state)\n",
    "  result.append(next_char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtwE5Slv2pun"
   },
   "outputs": [],
   "source": [
    "idxes_result = np.concatenate([ix[0].numpy() for ix in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fmoex5QW2zqi"
   },
   "outputs": [],
   "source": [
    "idxes_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Myg2t1XBzJgS"
   },
   "outputs": [],
   "source": [
    "''.join([tokens[idx] for idx in idxes_result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuRyCYggFOIh"
   },
   "outputs": [],
   "source": [
    "def generate_sample(one_step_model, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
    "                        smaller temperature converges to the single most likely output\n",
    "    '''\n",
    "    \n",
    "    #YOUR CODE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkpaDt1PFOIi"
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(one_step_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkoSFfJMFOIi"
   },
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    print(generate_sample(one_step_model, seed_phrase=' Sherl'.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "DFO-NFO2FOIi"
   },
   "source": [
    "### More seriously\n",
    "\n",
    "What we just did is a manual low-level implementation of RNN. While it's cool, i guess you won't like the idea of re-writing it from scratch on every occasion. \n",
    "\n",
    "As you might have guessed, tf has a solution for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR_btVxEFOIk"
   },
   "source": [
    "### Try it out!\n",
    "You've just implemented a recurrent language model that can be tasked with generating any kind of sequence, so there's plenty of data you can try it on:\n",
    "\n",
    "* Novels/poems/songs of your favorite author\n",
    "* News titles/clickbait titles\n",
    "* Source code of Linux or Tensorflow\n",
    "* Molecules in [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) format\n",
    "* Melody in notes/chords format\n",
    "* Ikea catalog titles\n",
    "* Pokemon names\n",
    "* Cards from Magic, the Gathering / Hearthstone\n",
    "\n",
    "If you're willing to give it a try, here's what you wanna look at:\n",
    "* Current data format is a sequence of lines, so a novel can be formatted as a list of sentences. Alternatively, you can change data preprocessing altogether.\n",
    "* While some datasets are readily available, others can only be scraped from the web. Try `Selenium` or `Scrapy` for that.\n",
    "* Make sure MAX_LENGTH is adjusted for longer datasets. There's also a bonus section about dynamic RNNs at the bottom.\n",
    "* More complex tasks require larger RNN architecture, try more neurons or several layers. It would also require more training iterations.\n",
    "* Long-term dependencies in music, novels or molecules are better handled with LSTM or GRU\n",
    "\n",
    "__Good hunting!__"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq_rnn_practice_tf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
