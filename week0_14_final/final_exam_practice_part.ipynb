{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvEt4K9MhuYa"
   },
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEJ1j2hzhwJ-"
   },
   "source": [
    "# Text classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ms9I7cS5h3Mr"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbyfn_zsh8Ft"
   },
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "The [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)  contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kLRsr4Bh-2i"
   },
   "source": [
    "### Download the IMDB dataset\n",
    "\n",
    "Let's download and extract the dataset, then explore the directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSR2hDlLh5HF",
    "outputId": "744aac3c-f635-4f2b-9a8a-d94960db436d"
   },
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEssbfKDjCEG",
    "outputId": "07c9981a-c657-4731-9cd3-bd227178ce35"
   },
   "outputs": [],
   "source": [
    "! apt-get install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycpN5rtTiF-x",
    "outputId": "7b99b0c5-adb8-4580-bd78-7dc607ac40ae"
   },
   "outputs": [],
   "source": [
    "! tree -L 2  $dataset_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0UiC49fi3Qb"
   },
   "source": [
    "The `aclImdb/train/pos` and `aclImdb/train/neg` directories contain many text files, each of which is a single movie review. Let's take a look at one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNaqTUMsixo0",
    "outputId": "56705664-06fb-482e-e240-8632edee9dd0"
   },
   "outputs": [],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
    "with open(sample_file) as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeD-1CBkjjoC"
   },
   "source": [
    "### Load the dataset\n",
    "\n",
    "Next,  load the data off disk and prepare it into a format suitable for training using [text_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory) utility, which expects a directory structure as follows.\n",
    "\n",
    "```\n",
    "main_directory/\n",
    "...class_a/\n",
    "......a_text_1.txt\n",
    "......a_text_2.txt\n",
    "...class_b/\n",
    "......b_text_1.txt\n",
    "......b_text_2.txt\n",
    "```\n",
    "\n",
    "This tool expects two folders on disk, corresponding to `class_a` and `class_b`. These will be the positive and negative movie reviews, which can be found in  `aclImdb/train/pos` and `aclImdb/train/neg`. Other folders must be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bck1A2r5jkpZ"
   },
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLXPDw2tj9Zg",
    "outputId": "fe90cb36-575e-4c41-a3e3-6b5503790440"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=None, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJINrQRVj_rT",
    "outputId": "ec44b266-69d7-4fcf-97ab-cd68a9c93b4b"
   },
   "outputs": [],
   "source": [
    "text, label = next(iter(raw_train_ds))\n",
    "print(\"Review\", text.numpy())\n",
    "print(\"Label\", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDut2-ZGkDZE"
   },
   "source": [
    "Notice the reviews contain raw text (with punctuation and occasional HTML tags like `<br/>`). You will show how to handle these in the following section. \n",
    "\n",
    "The labels are 0 or 1. To see which of these correspond to positive and negative movie reviews, you can check the `class_names` property on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyxuecUFkCqt",
    "outputId": "baa37067-a3ae-47b3-ddf6-c03f2ba82306"
   },
   "outputs": [],
   "source": [
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0_1Pv_pkLo9",
    "outputId": "d2a8fd12-7134-4643-abde-86394b9f3aad"
   },
   "outputs": [],
   "source": [
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=None, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5BhSSi5kNmN",
    "outputId": "52a0c0a9-016b-4700-89aa-fc77a6e4a193"
   },
   "outputs": [],
   "source": [
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', \n",
    "    batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfU0Uj_VkSmg"
   },
   "source": [
    "## Prepare the dataset for training (5 points)\n",
    "\n",
    "At this part, you should standardize, tokenize, and vectorize the data. \n",
    "\n",
    "As you saw above, the reviews contain various HTML tags like `<br />`. These tags will not be removed by the default standardizer in the `TextVectorization` layer (which converts text to lowercase and strips punctuation by default, but doesn't strip HTML). You can remove the HTML as well.\n",
    "\n",
    "It's a good idea to debug your vectorization on one scentence and then apply to all sentences using .map.\n",
    "\n",
    "Print several examples of the vectorization, size of the vocbulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2OK6uNMlCG3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owQTBAxZl0ji"
   },
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
    "\n",
    "`.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
    "\n",
    "`.prefetch()` overlaps data preprocessing and model execution while training. \n",
    "\n",
    "You can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CX6qU7SCl1Vf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcSdcmoSmMfi"
   },
   "source": [
    "## Create train and evaluate a model \n",
    "\n",
    "(5 points to any kind of bag of words, 10 for deep learning model that is using complicated features(like word embeddings) or processing data as a sequence). For complicated models If you will have low score (let's start with 0.8 accuracy on test) I can ask you to provide me with a baseline and prove your improvement and get 10 points.\n",
    "\n",
    "Create and train your model.\n",
    "Evaluate your model on test using all the known classification metics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJNt4I4Qm7QP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "final_exam_practice_part.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
